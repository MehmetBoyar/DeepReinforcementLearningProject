# ğŸš¦ Adaptive Traffic Signal Control (RL)

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0-ee4c2c)
![Streamlit](https://img.shields.io/badge/Streamlit-Dashboard-ff4b4b)
![License](https://img.shields.io/badge/License-MIT-green)

A comprehensive Research Framework for optimizing traffic signal phases using **Deep Reinforcement Learning (DQN)** and **Q-Learning**. 

This project features a fully interactive **Streamlit Dashboard** for training agents, analyzing performance metrics, visualizing real-time behavior, and comparing experimental results.

---

## ğŸŒŸ Features

### ğŸ–¥ï¸ Interactive Dashboard
A multi-page GUI to manage the entire lifecycle of the research project.
- **ğŸ‹ï¸ Train:** Launch batch experiments with varying traffic loads (Low, Med, High, Extreme).
- **ğŸ“Š Analysis:** View leaderboards, improvement heatmaps, and wait-time statistics.
- **ğŸ•µï¸ Deep Dive:** Replay episodes via **GIF visualization** and analyze fairness (queue distribution).
- **âš–ï¸ Compare:** Head-to-head comparison of two model versions (e.g., Baseline vs. Optimized).
- **ğŸ§ª Optimize:** Automated hyperparameter tuning using **Optuna**.

### ğŸ§  Intelligent Agents
- **Deep Q-Network (DQN):** Supports Double DQN, Dueling DQN, and Experience Replay.
- **Q-Learning:** Tabular RL approach for comparison.
- **Baselines:** Fixed-Time and Rule-Based Adaptive controllers.

### ğŸ“ˆ Monitoring
- **TensorBoard Integration:** Real-time tracking of Reward, Loss, and Average Queue Length.
- **Custom Metrics:** Tracks throughput, wait times, and lane fairness.

---

## ğŸ“¸ Screenshots

| Training Dashboard | Visual Replay |
|:---:|:---:|
| *(Add a screenshot of your Train page here)* | *(Add a GIF of your traffic simulation here)* |

---

## âš™ï¸ Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/yourusername/traffic-rl.git
   cd traffic-rl

2. **Create a Virtual Environment (Recommended)**
    # Windows
    python -m venv venv
    venv\Scripts\activate

    # Mac/Linux
    python3 -m venv venv
    source venv/bin/activate
pip install -r requirements.txt

## ğŸš€ How to Run
**Option 1: The Dashboard (Recommended)**

This launches the GUI in your default web browser.

streamlit run Traffic_RL.py

**Option 2: Command Line (CLI)**

You can also run experiments directly from the terminal.

# Train a single agent
python main.py train --config configs/default.yaml

# Run the full scientific batch (Low/Med/High traffic)
python main.py batch --out experiments_batch

## ğŸ“‚ Project Structure
```text
/
â”œâ”€â”€ Traffic_RL.py           # ğŸ  Main Dashboard Entry Point
â”œâ”€â”€ pages/                  # ğŸ“„ Streamlit Pages
â”‚   â”œâ”€â”€ 1_ğŸ‹ï¸_Train.py
â”‚   â”œâ”€â”€ 2_ğŸ“Š_Analysis.py
â”‚   â”œâ”€â”€ 3_ğŸ•µï¸_Deep_Dive.py
â”‚   â”œâ”€â”€ 4_âš–ï¸_Compare.py
â”‚   â””â”€â”€ 5_ğŸ§ª_Optimize.py
â”œâ”€â”€ traffic_rl/             # ğŸ“¦ Core Package
â”‚   â”œâ”€â”€ agents/             # RL Agent Logic (DQN, Q-Learning)
â”‚   â”œâ”€â”€ env/                # Gymnasium Environment (Traffic Logic)
â”‚   â””â”€â”€ tools/              # Analysis & Visualization Tools
â”œâ”€â”€ configs/                # âš™ï¸ Configuration Files
â”œâ”€â”€ experiments/            # ğŸ’¾ Saved Models & Logs
â””â”€â”€ requirements.txt        # Dependencies


## ğŸ› ï¸ Configuration

environment:
  traffic_multiplier: 1.0  # 1.0 = Normal, 2.0 = High Traffic
  max_steps: 1000          # Duration of one episode

agent:
  name: "dqn"
  lr: 0.001
  batch_size: 64
  hidden_dim: 128
  double_dqn: true         # Enable Double DQN stability

## ğŸ“ˆ TensorBoard

To view live training metrics (Loss, Reward, Queue Lengths):

    Click "ğŸš€ Launch TB" in the Dashboard Sidebar.

    Or run manually:
    code Bash

    tensorboard --logdir . --port 6006

    Open http://localhost:6006

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details. Code
---